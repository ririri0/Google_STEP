## メモ

- 問1に関してはワードに含まれる文字のそれぞれの個数について完全一致を探すため、文字列を並び替えたリストを用いた二分探索が計算量が最も少ない。今回は問2をベースに移植しているため、二分探索を用いていない。

- 問1に関して、例えばプログラムを何度も用いる場合、空間計算量を少なくするためには、words.txtを読み込んだ辞書用のメモリを事前に準備しておいて、繰り返し実行する際にはその辞書のアドレスにアクセスすると良い。辞書を収める領域が1つ(1回)で済む。それぞれの単語に含まれる文字の数が膨大である場合なども、二分探索するためにそれぞれの単語をソートして、またその単語のリスト全体をソートする計算が何度も必要なくなる。

- pythonでは値渡ししかできないのでそのようなプログラムは基本作れない。